{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5d9032-e409-4d34-9679-75398be95b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55980444-4912-453a-9e8c-aeb95d441bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "from loguru import logger\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b66b0a-41fa-46ee-96c7-baac2ab1710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_embedding import get_embedding\n",
    "from src.load_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c25c20f-effb-4ac2-b7ba-1519cb98af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"EuroBERT\": {\n",
    "        \"model\": AutoModel.from_pretrained(\n",
    "            \"EuroBERT/EuroBERT-210m\", trust_remote_code=True\n",
    "        ),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\n",
    "            \"EuroBERT/EuroBERT-210m\", trust_remote_code=True\n",
    "        ),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"EuroBERT_FT\": {\n",
    "        \"model\": AutoModel.from_pretrained(\n",
    "            \"nomic-ai/eurobert-210m-2e4-128sl-full-ft\", trust_remote_code=True\n",
    "        ),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\n",
    "            \"nomic-ai/eurobert-210m-2e4-128sl-full-ft\", trust_remote_code=True\n",
    "        ),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"RuModernBERT_USER2_FT\": {\n",
    "        \"model\": AutoModel.from_pretrained(\"deepvk/USER2-base\", trust_remote_code=True),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\n",
    "            \"deepvk/USER2-base\", trust_remote_code=True\n",
    "        ),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"LaBSE\": {\n",
    "        \"model\": AutoModel.from_pretrained(\"sentence-transformers/LaBSE\"),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\"),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"RuModernBERT\": {\n",
    "        \"model\": AutoModel.from_pretrained(\n",
    "            \"deepvk/RuModernBERT-base\", output_attentions=True, trust_remote_code=True\n",
    "        ),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\n",
    "            \"deepvk/RuModernBERT-base\", trust_remote_code=True\n",
    "        ),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"E5\": {\n",
    "        \"model\": AutoModel.from_pretrained(\"intfloat/multilingual-e5-base\"),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-base\"),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"MiniLM\": {\n",
    "        \"model\": SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\"),\n",
    "        \"type\": \"sentence-transformer\",\n",
    "    },\n",
    "    \"BERT-multilingual\": {\n",
    "        \"model\": AutoModel.from_pretrained(\"bert-base-multilingual-cased\"),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\"),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"gte-multilingual-base\": {\n",
    "        \"model\": AutoModel.from_pretrained(\n",
    "            \"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True\n",
    "        ),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\n",
    "            \"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True\n",
    "        ),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "    \"Granite-Embedding-Multilingual\": {\n",
    "        \"model\": AutoModel.from_pretrained(\n",
    "            \"ibm-granite/granite-embedding-278m-multilingual\"\n",
    "        ),\n",
    "        \"tokenizer\": AutoTokenizer.from_pretrained(\n",
    "            \"ibm-granite/granite-embedding-278m-multilingual\"\n",
    "        ),\n",
    "        \"type\": \"transformer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccc3b91-705f-4716-b29c-58eea0e88851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vector_index(model_type, tokenizer, model, support_passages):\n",
    "    support_embeddings = [\n",
    "        get_embedding(model_type=model_type, tokenizer=tokenizer, model=model, text=p)\n",
    "        for p in support_passages\n",
    "    ]  # use your current ModernBERT method\n",
    "    # Stack and normalize to unit vectors\n",
    "    support_embeddings_np = np.vstack(support_embeddings).astype(\"float32\")\n",
    "    support_embeddings_np = normalize(support_embeddings_np, norm=\"l2\", axis=1)\n",
    "\n",
    "    # FAISS IndexFlatL2 works as cosine similarity now\n",
    "    dimension = support_embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(support_embeddings_np)\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def retrieve_query(index, model_type, tokenizer, model, query, top_k=3):\n",
    "    try:\n",
    "        emb = get_embedding(\n",
    "            model_type=model_type, tokenizer=tokenizer, model=model, text=query\n",
    "        ).astype(\"float32\")\n",
    "    except:\n",
    "        logger.warning(f\"Could not embed query {query}\")\n",
    "        raise\n",
    "    D_a, I_a = index.search(np.array([emb]), top_k)\n",
    "    return df.iloc[I_a[0]]\n",
    "\n",
    "\n",
    "def compute_retrieval_results(\n",
    "    index, model_type, tokenizer, model, model_name, df, language, verbose=False\n",
    "):\n",
    "    retrieval_results = dict()\n",
    "    for i, row in df.iterrows():\n",
    "        row_id = row[\"id\"]\n",
    "        q_active = row[\"query_active\"]\n",
    "        q_passive = row[\"query_passive\"]\n",
    "        if verbose:\n",
    "            logger.info(f\"Retrieving for {q_active}, {q_passive}\")\n",
    "        retrieved_ids_active = retrieve_query(\n",
    "            index=index,\n",
    "            model_type=model_type,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            query=q_active,\n",
    "            top_k=top_k,\n",
    "        )[\"id\"].tolist()\n",
    "        retrieved_ids_passive = retrieve_query(\n",
    "            index=index,\n",
    "            model_type=model_type,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            query=q_passive,\n",
    "            top_k=top_k,\n",
    "        )[\"id\"].tolist()\n",
    "        active_passive_retrievals = {\n",
    "            \"active\": retrieved_ids_active,\n",
    "            \"passive\": retrieved_ids_passive,\n",
    "        }\n",
    "        retrieval_results[row_id] = active_passive_retrievals\n",
    "    with open(f\"results/retrieval_results_{language}_{model_name}.json\", \"w\") as f:\n",
    "        json.dump(retrieval_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5649fefc-cb94-43d3-8073-12ec0a33f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b5f99a-45ce-42d3-bb8c-3eee7e558286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-11 16:10:11.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING EuroBERT, en\u001b[0m\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[32m2025-06-11 16:11:11.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING EuroBERT, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:12:17.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING EuroBERT_FT, en\u001b[0m\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[32m2025-06-11 16:13:19.719\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING EuroBERT_FT, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:14:24.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING RuModernBERT_USER2_FT, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:15:30.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING RuModernBERT_USER2_FT, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:16:36.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING LaBSE, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:17:24.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING LaBSE, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:18:13.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING RuModernBERT, en\u001b[0m\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Outputting attentions is only supported with the 'eager' attention implementation, not with \"sdpa\". Falling back to `attn_implementation=\"eager\"`.\n",
      "\u001b[32m2025-06-11 16:19:21.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING RuModernBERT, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:20:29.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING E5, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:21:18.992\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING E5, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:22:07.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING MiniLM, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:22:25.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING MiniLM, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:22:44.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING BERT-multilingual, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:23:31.727\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING BERT-multilingual, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:24:20.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING gte-multilingual-base, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:25:19.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING gte-multilingual-base, ru\u001b[0m\n",
      "\u001b[32m2025-06-11 16:26:19.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING Granite-Embedding-Multilingual, en\u001b[0m\n",
      "\u001b[32m2025-06-11 16:27:04.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mPROCESSING Granite-Embedding-Multilingual, ru\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for model_name in models.keys():\n",
    "    for language in [\"en\", \"ru\"]:\n",
    "        logger.info(f\"PROCESSING {model_name}, {language}\")\n",
    "        df = load_data(language)\n",
    "        support_passages = df[\"support_passage\"].tolist()\n",
    "        loaded_models = models[model_name]\n",
    "        model = loaded_models[\"model\"]\n",
    "        tokenizer = loaded_models.get(\"tokenizer\", None)\n",
    "        model_type = loaded_models[\"type\"]\n",
    "        index = make_vector_index(\n",
    "            model_type=model_type,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            support_passages=support_passages,\n",
    "        )\n",
    "        compute_retrieval_results(\n",
    "            index=index,\n",
    "            model_type=model_type,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            df=df,\n",
    "            language=language,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
